{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loads a pretrained GPT-2 model and finetunes it to work for generating Python code.\n",
    "1. Download and load a pretrained version of GPT-2 small.\n",
    "2. Load data from the './data' directory.\n",
    "3. Clean the data and use the GPT-2 tokenizer to prepare the data for training.\n",
    "4. Split the data into training and validation sets.\n",
    "5. Train the model in a semi-supervised fashion with the following tasks:\n",
    "   - Feed the model a portion of the code for an example and have it predict the next token.\n",
    "6. Evaluate the model on the next token generation task, and report the metrics.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from dataset import CodeDataset\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "# Set the random seed for reproducibility.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "CodeDataset.init(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the './data' directory.\n",
    "# The data directory contains many more folders that contain the target python files\n",
    "# and the corresponding code snippets.\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk('./data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.py'):\n",
    "            data_files.append(os.path.join(root, file))\n",
    "\n",
    "data = data_files\n",
    "'Data len: {}'.format(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = CodeDataset(data)\n",
    "\n",
    "# Split the dataset into training and validation sets.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create a dataloader for the training and validation sets.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_dataloader):\n",
    "    print(data)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer.\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_ = \"cuda:0\"\n",
    "device = torch.device(cuda_ if torch.cuda.is_available() else \"cpu\")\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the latest file in aphabetical order in ./data directory\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.makedirs('./checkpoints')\n",
    "\n",
    "file_list = glob.glob('./checkpoints/*')\n",
    "print(file_list)\n",
    "\n",
    "checkpoint_path = ''\n",
    "_epoch, _i = 0, -1\n",
    "index = 0\n",
    "\n",
    "if len(file_list) > 0:\n",
    "    checkpoint_path = max(file_list, key=os.path.getctime)\n",
    "    print(checkpoint_path)\n",
    "\n",
    "    _base_name = os.path.basename(checkpoint_path)\n",
    "    _wihout_ext = os.path.splitext(_base_name)[0]\n",
    "    _tmp_args = _wihout_ext.split('-')\n",
    "    _epoch, _i = int(_tmp_args[0]), int(_tmp_args[1])\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print('Loaded Epoch {}s i {} checkpoint from {}'.format(_epoch, _i, checkpoint_path))\n",
    "    index = _i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(_epoch, 10):\n",
    "    for i, batch in enumerate(train_dataloader, start=index):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch[:, :1000]\n",
    "        output = model(batch, labels=batch)\n",
    "        loss = output[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save the model every 10 batche\n",
    "        if i % 10 == 0:\n",
    "            torch.save(model.state_dict(), './checkpoints/{}-{}.pt'.format(epoch, i))\n",
    "\n",
    "        print('Epoch: {}/{}'.format(epoch, 10),\n",
    "                'Step: {}/{}'.format(i, len(train_dataloader)),\n",
    "                'Loss: {}'.format(loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    batch = batch[:, :1000]\n",
    "    output = model(batch, labels=batch)\n",
    "    loss = output[0]\n",
    "    print('Epoch: {}/{}, Test Loss: {}'.format(epoch, 10, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./out'):\n",
    "    os.mkdir('./out')\n",
    "torch.save(model.state_dict(), './out/gpt2_code_generator.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.generate_code(\"import torch\\n\", length=100, temperature=0.7, top_k=50)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25d5de7a11a9be3c45ed68958e595ac9c7ba08c6a06e3781997497fbdff2d650"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
