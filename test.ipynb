{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirements\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from dataset import CodeDataset\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "# Set the random seed for reproducibility.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "CodeDataset.init(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data len: 150000'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the './data' directory.\n",
    "# The data directory contains many more folders that contain the target python files\n",
    "# and the corresponding code snippets.\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk('./data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.py'):\n",
    "            data_files.append(os.path.join(root, file))\n",
    "\n",
    "data = data_files\n",
    "'Data len: {}'.format(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = CodeDataset(data)\n",
    "\n",
    "# Split the dataset into training and validation sets.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create a dataloader for the training and validation sets.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2500 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[37811,  2025,  2134,  ...,  6015,  3419,   198]])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dataloader):\n",
    "    print(data)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer.\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_ = \"cuda:0\"\n",
    "device = torch.device(cuda_ if torch.cuda.is_available() else \"cpu\")\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./checkpoints/0-0.pt']\n",
      "./checkpoints/0-0.pt\n",
      "Loaded Epoch 0s i 0 checkpoint from ./checkpoints/0-0.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the latest file in aphabetical order in ./data directory\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.makedirs('./checkpoints')\n",
    "\n",
    "file_list = glob.glob('./checkpoints/*')\n",
    "print(file_list)\n",
    "\n",
    "checkpoint_path = ''\n",
    "_epoch, _i = 0, -1\n",
    "index = 0\n",
    "\n",
    "if len(file_list) > 0:\n",
    "    checkpoint_path = max(file_list, key=os.path.getctime)\n",
    "    print(checkpoint_path)\n",
    "\n",
    "    _base_name = os.path.basename(checkpoint_path)\n",
    "    _wihout_ext = os.path.splitext(_base_name)[0]\n",
    "    _tmp_args = _wihout_ext.split('-')\n",
    "    _epoch, _i = int(_tmp_args[0]), int(_tmp_args[1])\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print('Loaded Epoch {}s i {} checkpoint from {}'.format(_epoch, _i, checkpoint_path))\n",
    "    index = _i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Step: 1/135000 Loss: 2.7234344482421875\n",
      "Epoch: 0/10 Step: 2/135000 Loss: 2.135730266571045\n",
      "Epoch: 0/10 Step: 3/135000 Loss: 1.9999090433120728\n",
      "Epoch: 0/10 Step: 4/135000 Loss: 3.5905206203460693\n",
      "Epoch: 0/10 Step: 5/135000 Loss: 2.7974772453308105\n",
      "Epoch: 0/10 Step: 6/135000 Loss: 2.3403780460357666\n",
      "Epoch: 0/10 Step: 7/135000 Loss: 1.8329054117202759\n",
      "Epoch: 0/10 Step: 8/135000 Loss: 4.88843297958374\n",
      "Epoch: 0/10 Step: 9/135000 Loss: 2.6794021129608154\n",
      "Epoch: 0/10 Step: 10/135000 Loss: 2.762396812438965\n",
      "Epoch: 0/10 Step: 11/135000 Loss: 1.750596284866333\n",
      "Epoch: 0/10 Step: 12/135000 Loss: 1.7422051429748535\n",
      "Epoch: 0/10 Step: 13/135000 Loss: 1.3562179803848267\n",
      "Epoch: 0/10 Step: 14/135000 Loss: 4.167167663574219\n",
      "Epoch: 0/10 Step: 15/135000 Loss: 1.390671968460083\n",
      "Epoch: 0/10 Step: 16/135000 Loss: 1.8474671840667725\n",
      "Epoch: 0/10 Step: 17/135000 Loss: 3.7322046756744385\n",
      "Epoch: 0/10 Step: 18/135000 Loss: 2.188779592514038\n",
      "Epoch: 0/10 Step: 19/135000 Loss: 1.7991576194763184\n",
      "Epoch: 0/10 Step: 20/135000 Loss: 2.3017799854278564\n",
      "Epoch: 0/10 Step: 21/135000 Loss: 1.4089915752410889\n",
      "Epoch: 0/10 Step: 22/135000 Loss: 3.0351386070251465\n",
      "Epoch: 0/10 Step: 23/135000 Loss: 2.356309175491333\n",
      "Epoch: 0/10 Step: 24/135000 Loss: 2.076214551925659\n",
      "Epoch: 0/10 Step: 25/135000 Loss: 1.0052860975265503\n",
      "Epoch: 0/10 Step: 26/135000 Loss: 2.43234920501709\n",
      "Epoch: 0/10 Step: 27/135000 Loss: 1.8392285108566284\n",
      "Epoch: 0/10 Step: 28/135000 Loss: 3.565762519836426\n",
      "Epoch: 0/10 Step: 29/135000 Loss: 1.5193456411361694\n",
      "Epoch: 0/10 Step: 30/135000 Loss: 2.572619676589966\n",
      "Epoch: 0/10 Step: 31/135000 Loss: 2.3845713138580322\n",
      "Epoch: 0/10 Step: 32/135000 Loss: 6.129589557647705\n",
      "Epoch: 0/10 Step: 33/135000 Loss: 1.2264223098754883\n",
      "Epoch: 0/10 Step: 34/135000 Loss: 1.802164077758789\n",
      "Epoch: 0/10 Step: 35/135000 Loss: 2.4782891273498535\n",
      "Epoch: 0/10 Step: 36/135000 Loss: 1.0084859132766724\n",
      "Epoch: 0/10 Step: 37/135000 Loss: 2.2335994243621826\n",
      "Epoch: 0/10 Step: 38/135000 Loss: 1.6034818887710571\n",
      "Epoch: 0/10 Step: 39/135000 Loss: 1.5895367860794067\n",
      "Epoch: 0/10 Step: 40/135000 Loss: 2.0038416385650635\n",
      "Epoch: 0/10 Step: 41/135000 Loss: 1.805614709854126\n",
      "Epoch: 0/10 Step: 42/135000 Loss: 1.1957751512527466\n",
      "Epoch: 0/10 Step: 43/135000 Loss: 1.8351119756698608\n",
      "Epoch: 0/10 Step: 44/135000 Loss: 1.6986783742904663\n",
      "Epoch: 0/10 Step: 45/135000 Loss: 1.3632221221923828\n",
      "Epoch: 0/10 Step: 46/135000 Loss: 1.462934136390686\n",
      "Epoch: 0/10 Step: 47/135000 Loss: 0.9879891276359558\n",
      "Epoch: 0/10 Step: 48/135000 Loss: 3.46232533454895\n",
      "Epoch: 0/10 Step: 49/135000 Loss: 1.6269513368606567\n",
      "Epoch: 0/10 Step: 50/135000 Loss: 2.74530291557312\n",
      "Epoch: 0/10 Step: 51/135000 Loss: 1.822842001914978\n",
      "Epoch: 0/10 Step: 52/135000 Loss: 1.5922882556915283\n",
      "Epoch: 0/10 Step: 53/135000 Loss: 1.3639061450958252\n",
      "Epoch: 0/10 Step: 54/135000 Loss: 3.854024648666382\n",
      "Epoch: 0/10 Step: 55/135000 Loss: 1.726967692375183\n",
      "Epoch: 0/10 Step: 56/135000 Loss: 3.355656385421753\n",
      "Epoch: 0/10 Step: 57/135000 Loss: 2.1684820652008057\n",
      "Epoch: 0/10 Step: 58/135000 Loss: 1.834925651550293\n",
      "Epoch: 0/10 Step: 59/135000 Loss: 1.9865474700927734\n",
      "Epoch: 0/10 Step: 60/135000 Loss: 2.853381633758545\n",
      "Epoch: 0/10 Step: 61/135000 Loss: 2.042057752609253\n",
      "Epoch: 0/10 Step: 62/135000 Loss: 1.7792900800704956\n",
      "Epoch: 0/10 Step: 63/135000 Loss: 2.068082571029663\n",
      "Epoch: 0/10 Step: 64/135000 Loss: 1.728309154510498\n",
      "Epoch: 0/10 Step: 65/135000 Loss: 2.1183314323425293\n",
      "Epoch: 0/10 Step: 66/135000 Loss: 0.9219886660575867\n",
      "Epoch: 0/10 Step: 67/135000 Loss: 1.5518051385879517\n",
      "Epoch: 0/10 Step: 68/135000 Loss: 1.1297956705093384\n",
      "Epoch: 0/10 Step: 69/135000 Loss: 1.928179383277893\n",
      "Epoch: 0/10 Step: 70/135000 Loss: 3.1897060871124268\n",
      "Epoch: 0/10 Step: 71/135000 Loss: 2.011806011199951\n",
      "Epoch: 0/10 Step: 72/135000 Loss: 1.7792524099349976\n",
      "Epoch: 0/10 Step: 73/135000 Loss: 1.736149549484253\n",
      "Epoch: 0/10 Step: 74/135000 Loss: 1.6042579412460327\n",
      "Epoch: 0/10 Step: 75/135000 Loss: 1.5130679607391357\n",
      "Epoch: 0/10 Step: 76/135000 Loss: 3.1772677898406982\n",
      "Epoch: 0/10 Step: 77/135000 Loss: 2.561070203781128\n",
      "Epoch: 0/10 Step: 78/135000 Loss: 2.462920665740967\n",
      "Epoch: 0/10 Step: 79/135000 Loss: 2.5284979343414307\n",
      "Epoch: 0/10 Step: 80/135000 Loss: 1.713384747505188\n",
      "Epoch: 0/10 Step: 81/135000 Loss: 1.9119455814361572\n",
      "Epoch: 0/10 Step: 82/135000 Loss: 2.3182523250579834\n",
      "Epoch: 0/10 Step: 83/135000 Loss: 2.6477339267730713\n",
      "Epoch: 0/10 Step: 84/135000 Loss: 1.648736596107483\n",
      "Epoch: 0/10 Step: 85/135000 Loss: 2.927377223968506\n",
      "Epoch: 0/10 Step: 86/135000 Loss: 2.588289499282837\n",
      "Epoch: 0/10 Step: 87/135000 Loss: 1.790128231048584\n",
      "Epoch: 0/10 Step: 88/135000 Loss: 1.644433856010437\n",
      "Epoch: 0/10 Step: 89/135000 Loss: 2.021779775619507\n",
      "Epoch: 0/10 Step: 90/135000 Loss: 1.7550866603851318\n",
      "Epoch: 0/10 Step: 91/135000 Loss: 1.8765250444412231\n",
      "Epoch: 0/10 Step: 92/135000 Loss: 2.146380662918091\n",
      "Epoch: 0/10 Step: 93/135000 Loss: 1.637744665145874\n",
      "Epoch: 0/10 Step: 94/135000 Loss: 2.318876266479492\n",
      "Epoch: 0/10 Step: 95/135000 Loss: 2.5372607707977295\n",
      "Epoch: 0/10 Step: 96/135000 Loss: 3.883033275604248\n",
      "Epoch: 0/10 Step: 97/135000 Loss: 1.7246345281600952\n",
      "Epoch: 0/10 Step: 98/135000 Loss: 1.22645103931427\n",
      "Epoch: 0/10 Step: 99/135000 Loss: 2.1114799976348877\n",
      "Epoch: 0/10 Step: 100/135000 Loss: 2.2711246013641357\n",
      "Epoch: 0/10 Step: 101/135000 Loss: 1.8296329975128174\n",
      "Epoch: 0/10 Step: 102/135000 Loss: 1.4074937105178833\n",
      "Epoch: 0/10 Step: 103/135000 Loss: 2.4857017993927\n",
      "Epoch: 0/10 Step: 104/135000 Loss: 2.079204797744751\n",
      "Epoch: 0/10 Step: 105/135000 Loss: 2.881420373916626\n",
      "Epoch: 0/10 Step: 106/135000 Loss: 1.6946382522583008\n",
      "Epoch: 0/10 Step: 107/135000 Loss: 1.1677496433258057\n",
      "Epoch: 0/10 Step: 108/135000 Loss: 1.6049402952194214\n",
      "Epoch: 0/10 Step: 109/135000 Loss: 1.7801681756973267\n",
      "Epoch: 0/10 Step: 110/135000 Loss: 2.3036880493164062\n",
      "Epoch: 0/10 Step: 111/135000 Loss: 3.366795063018799\n",
      "Epoch: 0/10 Step: 112/135000 Loss: 1.4107376337051392\n",
      "Epoch: 0/10 Step: 113/135000 Loss: 1.9144892692565918\n",
      "Epoch: 0/10 Step: 114/135000 Loss: 2.313713312149048\n",
      "Epoch: 0/10 Step: 115/135000 Loss: 2.151765823364258\n",
      "Epoch: 0/10 Step: 116/135000 Loss: 2.161602258682251\n",
      "Epoch: 0/10 Step: 117/135000 Loss: 2.229536533355713\n",
      "Epoch: 0/10 Step: 118/135000 Loss: 2.1940417289733887\n",
      "Epoch: 0/10 Step: 119/135000 Loss: 2.029872179031372\n",
      "Epoch: 0/10 Step: 120/135000 Loss: 1.0953657627105713\n",
      "Epoch: 0/10 Step: 121/135000 Loss: 2.2965307235717773\n",
      "Epoch: 0/10 Step: 122/135000 Loss: 1.8305342197418213\n",
      "Epoch: 0/10 Step: 123/135000 Loss: 2.7455830574035645\n",
      "Epoch: 0/10 Step: 124/135000 Loss: 2.237471103668213\n",
      "Epoch: 0/10 Step: 125/135000 Loss: 1.9189281463623047\n",
      "Epoch: 0/10 Step: 126/135000 Loss: 2.571657657623291\n",
      "Epoch: 0/10 Step: 127/135000 Loss: 1.5845422744750977\n",
      "Epoch: 0/10 Step: 128/135000 Loss: 2.07761549949646\n",
      "Epoch: 0/10 Step: 129/135000 Loss: 2.294632911682129\n",
      "Epoch: 0/10 Step: 130/135000 Loss: 1.5570601224899292\n",
      "Epoch: 0/10 Step: 131/135000 Loss: 2.989053726196289\n",
      "Epoch: 0/10 Step: 132/135000 Loss: 2.2364587783813477\n",
      "Epoch: 0/10 Step: 133/135000 Loss: 1.9038712978363037\n",
      "Epoch: 0/10 Step: 134/135000 Loss: 3.2472915649414062\n",
      "Epoch: 0/10 Step: 135/135000 Loss: 1.678023099899292\n",
      "Epoch: 0/10 Step: 136/135000 Loss: 2.3155934810638428\n",
      "Epoch: 0/10 Step: 137/135000 Loss: 1.7940958738327026\n",
      "Epoch: 0/10 Step: 138/135000 Loss: 1.445947289466858\n",
      "Epoch: 0/10 Step: 139/135000 Loss: 2.418386697769165\n",
      "Epoch: 0/10 Step: 140/135000 Loss: 1.6181546449661255\n",
      "Epoch: 0/10 Step: 141/135000 Loss: 2.0985119342803955\n",
      "Epoch: 0/10 Step: 142/135000 Loss: 1.6464347839355469\n",
      "Epoch: 0/10 Step: 143/135000 Loss: 3.946951389312744\n",
      "Epoch: 0/10 Step: 144/135000 Loss: 0.8586540222167969\n",
      "Epoch: 0/10 Step: 145/135000 Loss: 2.0248100757598877\n",
      "Epoch: 0/10 Step: 146/135000 Loss: 2.2843852043151855\n",
      "Epoch: 0/10 Step: 147/135000 Loss: 1.2113755941390991\n",
      "Epoch: 0/10 Step: 148/135000 Loss: 2.454268217086792\n",
      "Epoch: 0/10 Step: 149/135000 Loss: 3.1621899604797363\n",
      "Epoch: 0/10 Step: 150/135000 Loss: 1.3205006122589111\n",
      "Epoch: 0/10 Step: 151/135000 Loss: 1.5746041536331177\n",
      "Epoch: 0/10 Step: 152/135000 Loss: 2.0996837615966797\n",
      "Epoch: 0/10 Step: 153/135000 Loss: 1.8636144399642944\n",
      "Epoch: 0/10 Step: 154/135000 Loss: 2.1989524364471436\n",
      "Epoch: 0/10 Step: 155/135000 Loss: 1.3271085023880005\n",
      "Epoch: 0/10 Step: 156/135000 Loss: 1.5742509365081787\n",
      "Epoch: 0/10 Step: 157/135000 Loss: 1.3989834785461426\n",
      "Epoch: 0/10 Step: 158/135000 Loss: 2.5200984477996826\n",
      "Epoch: 0/10 Step: 159/135000 Loss: 1.7416545152664185\n",
      "Epoch: 0/10 Step: 160/135000 Loss: 1.8581537008285522\n",
      "Epoch: 0/10 Step: 161/135000 Loss: 1.9045495986938477\n",
      "Epoch: 0/10 Step: 162/135000 Loss: 1.3292274475097656\n",
      "Epoch: 0/10 Step: 163/135000 Loss: 1.99761164188385\n",
      "Epoch: 0/10 Step: 164/135000 Loss: 1.3395771980285645\n",
      "Epoch: 0/10 Step: 165/135000 Loss: 1.8009002208709717\n",
      "Epoch: 0/10 Step: 166/135000 Loss: 3.285954475402832\n",
      "Epoch: 0/10 Step: 167/135000 Loss: 1.9416639804840088\n",
      "Epoch: 0/10 Step: 168/135000 Loss: 2.1143271923065186\n",
      "Epoch: 0/10 Step: 169/135000 Loss: 1.6412433385849\n",
      "Epoch: 0/10 Step: 170/135000 Loss: 2.236004590988159\n",
      "Epoch: 0/10 Step: 171/135000 Loss: 2.0891594886779785\n",
      "Epoch: 0/10 Step: 172/135000 Loss: 2.1138947010040283\n",
      "Epoch: 0/10 Step: 173/135000 Loss: 2.5370519161224365\n",
      "Epoch: 0/10 Step: 174/135000 Loss: 1.591139793395996\n",
      "Epoch: 0/10 Step: 175/135000 Loss: 1.1416103839874268\n",
      "Epoch: 0/10 Step: 176/135000 Loss: 2.4675328731536865\n",
      "Epoch: 0/10 Step: 177/135000 Loss: 2.58414888381958\n",
      "Epoch: 0/10 Step: 178/135000 Loss: 1.0751490592956543\n",
      "Epoch: 0/10 Step: 179/135000 Loss: 2.763564109802246\n",
      "Epoch: 0/10 Step: 180/135000 Loss: 1.9794858694076538\n",
      "Epoch: 0/10 Step: 181/135000 Loss: 1.2302860021591187\n",
      "Epoch: 0/10 Step: 182/135000 Loss: 2.008124589920044\n",
      "Epoch: 0/10 Step: 183/135000 Loss: 2.165696144104004\n",
      "Epoch: 0/10 Step: 184/135000 Loss: 1.5559688806533813\n",
      "Epoch: 0/10 Step: 185/135000 Loss: 1.1880104541778564\n",
      "Epoch: 0/10 Step: 186/135000 Loss: 2.7605252265930176\n",
      "Epoch: 0/10 Step: 187/135000 Loss: 2.4021332263946533\n",
      "Epoch: 0/10 Step: 188/135000 Loss: 1.745609998703003\n",
      "Epoch: 0/10 Step: 189/135000 Loss: 1.580845594406128\n",
      "Epoch: 0/10 Step: 190/135000 Loss: 1.9597471952438354\n",
      "Epoch: 0/10 Step: 191/135000 Loss: 2.2678613662719727\n",
      "Epoch: 0/10 Step: 192/135000 Loss: 1.903548240661621\n",
      "Epoch: 0/10 Step: 193/135000 Loss: 1.2732826471328735\n",
      "Epoch: 0/10 Step: 194/135000 Loss: 1.7746258974075317\n",
      "Epoch: 0/10 Step: 195/135000 Loss: 1.3562945127487183\n",
      "Epoch: 0/10 Step: 196/135000 Loss: 2.7572996616363525\n",
      "Epoch: 0/10 Step: 197/135000 Loss: 1.9511713981628418\n",
      "Epoch: 0/10 Step: 198/135000 Loss: 2.30000376701355\n",
      "Epoch: 0/10 Step: 199/135000 Loss: 1.9431374073028564\n",
      "Epoch: 0/10 Step: 200/135000 Loss: 1.2828315496444702\n",
      "Epoch: 0/10 Step: 201/135000 Loss: 2.9099419116973877\n",
      "Epoch: 0/10 Step: 202/135000 Loss: 3.190424919128418\n",
      "Epoch: 0/10 Step: 203/135000 Loss: 1.1006933450698853\n",
      "Epoch: 0/10 Step: 204/135000 Loss: 2.0193893909454346\n",
      "Epoch: 0/10 Step: 205/135000 Loss: 1.7861729860305786\n",
      "Epoch: 0/10 Step: 206/135000 Loss: 3.5571935176849365\n",
      "Epoch: 0/10 Step: 207/135000 Loss: 1.7738090753555298\n",
      "Epoch: 0/10 Step: 208/135000 Loss: 1.7612113952636719\n",
      "Epoch: 0/10 Step: 209/135000 Loss: 2.2115399837493896\n",
      "Epoch: 0/10 Step: 210/135000 Loss: 2.1512601375579834\n",
      "Epoch: 0/10 Step: 211/135000 Loss: 5.715991020202637\n",
      "Epoch: 0/10 Step: 212/135000 Loss: 2.0502138137817383\n",
      "Epoch: 0/10 Step: 213/135000 Loss: 2.126918077468872\n",
      "Epoch: 0/10 Step: 214/135000 Loss: 1.2773441076278687\n",
      "Epoch: 0/10 Step: 215/135000 Loss: 0.7049781084060669\n",
      "Epoch: 0/10 Step: 216/135000 Loss: 2.102832078933716\n",
      "Epoch: 0/10 Step: 217/135000 Loss: 2.703220844268799\n",
      "Epoch: 0/10 Step: 218/135000 Loss: 2.054456949234009\n",
      "Epoch: 0/10 Step: 219/135000 Loss: 1.3274033069610596\n",
      "Epoch: 0/10 Step: 220/135000 Loss: 1.4254844188690186\n",
      "Epoch: 0/10 Step: 221/135000 Loss: 1.8576158285140991\n",
      "Epoch: 0/10 Step: 222/135000 Loss: 1.6258490085601807\n",
      "Epoch: 0/10 Step: 223/135000 Loss: 2.157358169555664\n",
      "Epoch: 0/10 Step: 224/135000 Loss: 2.4902384281158447\n",
      "Epoch: 0/10 Step: 225/135000 Loss: 2.428819417953491\n",
      "Epoch: 0/10 Step: 226/135000 Loss: 2.2044460773468018\n",
      "Epoch: 0/10 Step: 227/135000 Loss: 2.248007297515869\n",
      "Epoch: 0/10 Step: 228/135000 Loss: 1.078761100769043\n",
      "Epoch: 0/10 Step: 229/135000 Loss: 1.9824646711349487\n",
      "Epoch: 0/10 Step: 230/135000 Loss: 1.8015660047531128\n",
      "Epoch: 0/10 Step: 231/135000 Loss: 2.007068395614624\n",
      "Epoch: 0/10 Step: 232/135000 Loss: 1.765707015991211\n",
      "Epoch: 0/10 Step: 233/135000 Loss: 1.8963892459869385\n",
      "Epoch: 0/10 Step: 234/135000 Loss: 1.700745940208435\n",
      "Epoch: 0/10 Step: 235/135000 Loss: 2.819071054458618\n",
      "Epoch: 0/10 Step: 236/135000 Loss: 1.821846604347229\n",
      "Epoch: 0/10 Step: 237/135000 Loss: 1.3389314413070679\n",
      "Epoch: 0/10 Step: 238/135000 Loss: 1.263683795928955\n",
      "Epoch: 0/10 Step: 239/135000 Loss: 1.703969120979309\n",
      "Epoch: 0/10 Step: 240/135000 Loss: 1.699328899383545\n",
      "Epoch: 0/10 Step: 241/135000 Loss: 2.0036885738372803\n",
      "Epoch: 0/10 Step: 242/135000 Loss: 2.3765149116516113\n",
      "Epoch: 0/10 Step: 243/135000 Loss: 1.9545209407806396\n",
      "Epoch: 0/10 Step: 244/135000 Loss: 1.7084522247314453\n",
      "Epoch: 0/10 Step: 245/135000 Loss: 1.1031386852264404\n",
      "Epoch: 0/10 Step: 246/135000 Loss: 1.5610432624816895\n",
      "Epoch: 0/10 Step: 247/135000 Loss: 2.8567593097686768\n",
      "Epoch: 0/10 Step: 248/135000 Loss: 1.9489705562591553\n",
      "Epoch: 0/10 Step: 249/135000 Loss: 3.1875832080841064\n",
      "Epoch: 0/10 Step: 250/135000 Loss: 1.4964350461959839\n",
      "Epoch: 0/10 Step: 251/135000 Loss: 2.2428009510040283\n",
      "Epoch: 0/10 Step: 252/135000 Loss: 2.0493690967559814\n",
      "Epoch: 0/10 Step: 253/135000 Loss: 1.43168044090271\n",
      "Epoch: 0/10 Step: 254/135000 Loss: 1.7347100973129272\n",
      "Epoch: 0/10 Step: 255/135000 Loss: 1.0622708797454834\n",
      "Epoch: 0/10 Step: 256/135000 Loss: 2.6070308685302734\n",
      "Epoch: 0/10 Step: 257/135000 Loss: 1.7762519121170044\n",
      "Epoch: 0/10 Step: 258/135000 Loss: 2.3884165287017822\n",
      "Epoch: 0/10 Step: 259/135000 Loss: 2.316340446472168\n",
      "Epoch: 0/10 Step: 260/135000 Loss: 2.294377088546753\n",
      "Epoch: 0/10 Step: 261/135000 Loss: 1.2319947481155396\n",
      "Epoch: 0/10 Step: 262/135000 Loss: 1.054686427116394\n",
      "Epoch: 0/10 Step: 263/135000 Loss: 2.0009729862213135\n",
      "Epoch: 0/10 Step: 264/135000 Loss: 2.6004230976104736\n",
      "Epoch: 0/10 Step: 265/135000 Loss: 1.683947205543518\n",
      "Epoch: 0/10 Step: 266/135000 Loss: 1.0828489065170288\n",
      "Epoch: 0/10 Step: 267/135000 Loss: 1.4697470664978027\n",
      "Epoch: 0/10 Step: 268/135000 Loss: 1.6504898071289062\n",
      "Epoch: 0/10 Step: 269/135000 Loss: 2.469055652618408\n",
      "Epoch: 0/10 Step: 270/135000 Loss: 2.5488340854644775\n",
      "Epoch: 0/10 Step: 271/135000 Loss: 2.262540817260742\n",
      "Epoch: 0/10 Step: 272/135000 Loss: 2.1318585872650146\n",
      "Epoch: 0/10 Step: 273/135000 Loss: 2.89882493019104\n",
      "Epoch: 0/10 Step: 274/135000 Loss: 1.455755352973938\n",
      "Epoch: 0/10 Step: 275/135000 Loss: 2.1403071880340576\n",
      "Epoch: 0/10 Step: 276/135000 Loss: 1.3992024660110474\n",
      "Epoch: 0/10 Step: 277/135000 Loss: 1.1826908588409424\n",
      "Epoch: 0/10 Step: 278/135000 Loss: 2.2514898777008057\n",
      "Epoch: 0/10 Step: 279/135000 Loss: 1.811923861503601\n",
      "Epoch: 0/10 Step: 280/135000 Loss: 1.3627121448516846\n",
      "Epoch: 0/10 Step: 281/135000 Loss: 1.7478598356246948\n",
      "Epoch: 0/10 Step: 282/135000 Loss: 3.3120474815368652\n",
      "Epoch: 0/10 Step: 283/135000 Loss: 1.0953725576400757\n",
      "Epoch: 0/10 Step: 284/135000 Loss: 1.0615143775939941\n",
      "Epoch: 0/10 Step: 285/135000 Loss: 1.8683507442474365\n",
      "Epoch: 0/10 Step: 286/135000 Loss: 1.4631141424179077\n",
      "Epoch: 0/10 Step: 287/135000 Loss: 2.382114887237549\n",
      "Epoch: 0/10 Step: 288/135000 Loss: 0.9854089021682739\n",
      "Epoch: 0/10 Step: 289/135000 Loss: 2.375743865966797\n",
      "Epoch: 0/10 Step: 290/135000 Loss: 1.6915966272354126\n",
      "Epoch: 0/10 Step: 291/135000 Loss: 2.441924571990967\n",
      "Epoch: 0/10 Step: 292/135000 Loss: 0.7908684015274048\n",
      "Epoch: 0/10 Step: 293/135000 Loss: 3.9748401641845703\n",
      "Epoch: 0/10 Step: 294/135000 Loss: 1.5488544702529907\n",
      "Epoch: 0/10 Step: 295/135000 Loss: 2.1906161308288574\n",
      "Epoch: 0/10 Step: 296/135000 Loss: 1.5444170236587524\n",
      "Epoch: 0/10 Step: 297/135000 Loss: 2.2067601680755615\n",
      "Epoch: 0/10 Step: 298/135000 Loss: 1.685176134109497\n",
      "Epoch: 0/10 Step: 299/135000 Loss: 1.7203755378723145\n",
      "Epoch: 0/10 Step: 300/135000 Loss: 1.719560980796814\n",
      "Epoch: 0/10 Step: 301/135000 Loss: 2.4555959701538086\n",
      "Epoch: 0/10 Step: 302/135000 Loss: 1.7911908626556396\n",
      "Epoch: 0/10 Step: 303/135000 Loss: 1.2463432550430298\n",
      "Epoch: 0/10 Step: 304/135000 Loss: 2.209853410720825\n",
      "Epoch: 0/10 Step: 305/135000 Loss: 2.382418155670166\n",
      "Epoch: 0/10 Step: 306/135000 Loss: 0.9924541115760803\n",
      "Epoch: 0/10 Step: 307/135000 Loss: 2.3897457122802734\n",
      "Epoch: 0/10 Step: 308/135000 Loss: 1.1137038469314575\n",
      "Epoch: 0/10 Step: 309/135000 Loss: 1.5985894203186035\n",
      "Epoch: 0/10 Step: 310/135000 Loss: 1.3337043523788452\n",
      "Epoch: 0/10 Step: 311/135000 Loss: 1.7354179620742798\n",
      "Epoch: 0/10 Step: 312/135000 Loss: 1.9811866283416748\n",
      "Epoch: 0/10 Step: 313/135000 Loss: 1.890023946762085\n",
      "Epoch: 0/10 Step: 314/135000 Loss: 1.7311944961547852\n",
      "Epoch: 0/10 Step: 315/135000 Loss: 4.046146869659424\n",
      "Epoch: 0/10 Step: 316/135000 Loss: 2.019749164581299\n",
      "Epoch: 0/10 Step: 317/135000 Loss: 1.2603110074996948\n",
      "Epoch: 0/10 Step: 318/135000 Loss: 1.9022457599639893\n",
      "Epoch: 0/10 Step: 319/135000 Loss: 1.4205631017684937\n",
      "Epoch: 0/10 Step: 320/135000 Loss: 1.0489628314971924\n",
      "Epoch: 0/10 Step: 321/135000 Loss: 1.721948504447937\n",
      "Epoch: 0/10 Step: 322/135000 Loss: 1.1109650135040283\n",
      "Epoch: 0/10 Step: 323/135000 Loss: 0.920684814453125\n",
      "Epoch: 0/10 Step: 324/135000 Loss: 1.7695250511169434\n",
      "Epoch: 0/10 Step: 325/135000 Loss: 1.7117253541946411\n",
      "Epoch: 0/10 Step: 326/135000 Loss: 3.6515684127807617\n",
      "Epoch: 0/10 Step: 327/135000 Loss: 1.7893345355987549\n",
      "Epoch: 0/10 Step: 328/135000 Loss: 1.8098558187484741\n",
      "Epoch: 0/10 Step: 329/135000 Loss: 1.5685449838638306\n",
      "Epoch: 0/10 Step: 330/135000 Loss: 1.7456018924713135\n",
      "Epoch: 0/10 Step: 331/135000 Loss: 1.2750746011734009\n",
      "Epoch: 0/10 Step: 332/135000 Loss: 1.830132007598877\n",
      "Epoch: 0/10 Step: 333/135000 Loss: 1.8187779188156128\n",
      "Epoch: 0/10 Step: 334/135000 Loss: 1.4244464635849\n",
      "Epoch: 0/10 Step: 335/135000 Loss: 1.0210518836975098\n",
      "Epoch: 0/10 Step: 336/135000 Loss: 0.7999476194381714\n",
      "Epoch: 0/10 Step: 337/135000 Loss: 1.5492138862609863\n",
      "Epoch: 0/10 Step: 338/135000 Loss: 2.9723141193389893\n",
      "Epoch: 0/10 Step: 339/135000 Loss: 2.257852077484131\n",
      "Epoch: 0/10 Step: 340/135000 Loss: 1.500539779663086\n",
      "Epoch: 0/10 Step: 341/135000 Loss: 1.433019757270813\n",
      "Epoch: 0/10 Step: 342/135000 Loss: 2.298793077468872\n",
      "Epoch: 0/10 Step: 343/135000 Loss: 1.405354619026184\n",
      "Epoch: 0/10 Step: 344/135000 Loss: 2.6728813648223877\n",
      "Epoch: 0/10 Step: 345/135000 Loss: 1.816454529762268\n",
      "Epoch: 0/10 Step: 346/135000 Loss: 1.2460163831710815\n",
      "Epoch: 0/10 Step: 347/135000 Loss: 0.8252056837081909\n",
      "Epoch: 0/10 Step: 348/135000 Loss: 2.9783053398132324\n",
      "Epoch: 0/10 Step: 349/135000 Loss: 1.2594196796417236\n",
      "Epoch: 0/10 Step: 350/135000 Loss: 1.1177762746810913\n",
      "Epoch: 0/10 Step: 351/135000 Loss: 2.186573028564453\n",
      "Epoch: 0/10 Step: 352/135000 Loss: 1.9006519317626953\n",
      "Epoch: 0/10 Step: 353/135000 Loss: 0.9976459741592407\n",
      "Epoch: 0/10 Step: 354/135000 Loss: 1.9330259561538696\n",
      "Epoch: 0/10 Step: 355/135000 Loss: 2.295501708984375\n",
      "Epoch: 0/10 Step: 356/135000 Loss: 1.7962960004806519\n",
      "Epoch: 0/10 Step: 357/135000 Loss: 1.8214643001556396\n",
      "Epoch: 0/10 Step: 358/135000 Loss: 1.4560551643371582\n",
      "Epoch: 0/10 Step: 359/135000 Loss: 1.1288518905639648\n",
      "Epoch: 0/10 Step: 360/135000 Loss: 2.0196194648742676\n",
      "Epoch: 0/10 Step: 361/135000 Loss: 1.6641110181808472\n",
      "Epoch: 0/10 Step: 362/135000 Loss: 1.6577117443084717\n",
      "Epoch: 0/10 Step: 363/135000 Loss: 0.984398603439331\n",
      "Epoch: 0/10 Step: 364/135000 Loss: 2.1532654762268066\n",
      "Epoch: 0/10 Step: 365/135000 Loss: 1.5006874799728394\n",
      "Epoch: 0/10 Step: 366/135000 Loss: 1.3911100625991821\n",
      "Epoch: 0/10 Step: 367/135000 Loss: 1.7659547328948975\n",
      "Epoch: 0/10 Step: 368/135000 Loss: 2.458313465118408\n",
      "Epoch: 0/10 Step: 369/135000 Loss: 2.166433334350586\n",
      "Epoch: 0/10 Step: 370/135000 Loss: 1.8727375268936157\n",
      "Epoch: 0/10 Step: 371/135000 Loss: 1.695212721824646\n",
      "Epoch: 0/10 Step: 372/135000 Loss: 1.3152153491973877\n",
      "Epoch: 0/10 Step: 373/135000 Loss: 0.9781456589698792\n",
      "Epoch: 0/10 Step: 374/135000 Loss: 2.592618942260742\n",
      "Epoch: 0/10 Step: 375/135000 Loss: 2.0838186740875244\n",
      "Epoch: 0/10 Step: 376/135000 Loss: 1.4114323854446411\n",
      "Epoch: 0/10 Step: 377/135000 Loss: 1.3693478107452393\n",
      "Epoch: 0/10 Step: 378/135000 Loss: 1.7871507406234741\n",
      "Epoch: 0/10 Step: 379/135000 Loss: 0.8760193586349487\n",
      "Epoch: 0/10 Step: 380/135000 Loss: 1.2014741897583008\n",
      "Epoch: 0/10 Step: 381/135000 Loss: 1.9734625816345215\n",
      "Epoch: 0/10 Step: 382/135000 Loss: 1.4844450950622559\n",
      "Epoch: 0/10 Step: 383/135000 Loss: 0.8175294995307922\n",
      "Epoch: 0/10 Step: 384/135000 Loss: 1.4764864444732666\n",
      "Epoch: 0/10 Step: 385/135000 Loss: 1.5639984607696533\n",
      "Epoch: 0/10 Step: 386/135000 Loss: 1.064931035041809\n",
      "Epoch: 0/10 Step: 387/135000 Loss: 0.8989177942276001\n",
      "Epoch: 0/10 Step: 388/135000 Loss: 1.4020864963531494\n",
      "Epoch: 0/10 Step: 389/135000 Loss: 1.608322024345398\n",
      "Epoch: 0/10 Step: 390/135000 Loss: 1.929911732673645\n",
      "Epoch: 0/10 Step: 391/135000 Loss: 1.9720922708511353\n",
      "Epoch: 0/10 Step: 392/135000 Loss: 2.5653154850006104\n",
      "Epoch: 0/10 Step: 393/135000 Loss: 2.0991265773773193\n",
      "Epoch: 0/10 Step: 394/135000 Loss: 1.8399995565414429\n",
      "Epoch: 0/10 Step: 395/135000 Loss: 1.7931462526321411\n",
      "Epoch: 0/10 Step: 396/135000 Loss: 0.7950025200843811\n",
      "Epoch: 0/10 Step: 397/135000 Loss: 1.8316903114318848\n",
      "Epoch: 0/10 Step: 398/135000 Loss: 1.364764928817749\n",
      "Epoch: 0/10 Step: 399/135000 Loss: 1.6611052751541138\n",
      "Epoch: 0/10 Step: 400/135000 Loss: 1.880844235420227\n",
      "Epoch: 0/10 Step: 401/135000 Loss: 1.2633953094482422\n",
      "Epoch: 0/10 Step: 402/135000 Loss: 1.5107331275939941\n",
      "Epoch: 0/10 Step: 403/135000 Loss: 0.819986879825592\n",
      "Epoch: 0/10 Step: 404/135000 Loss: 0.6978469491004944\n",
      "Epoch: 0/10 Step: 405/135000 Loss: 2.6638879776000977\n",
      "Epoch: 0/10 Step: 406/135000 Loss: 2.8019638061523438\n",
      "Epoch: 0/10 Step: 407/135000 Loss: 0.8299151062965393\n",
      "Epoch: 0/10 Step: 408/135000 Loss: 2.1409788131713867\n",
      "Epoch: 0/10 Step: 409/135000 Loss: 1.0886626243591309\n",
      "Epoch: 0/10 Step: 410/135000 Loss: 1.1493682861328125\n",
      "Epoch: 0/10 Step: 411/135000 Loss: 2.5275707244873047\n",
      "Epoch: 0/10 Step: 412/135000 Loss: 1.5801795721054077\n",
      "Epoch: 0/10 Step: 413/135000 Loss: 1.7209733724594116\n",
      "Epoch: 0/10 Step: 414/135000 Loss: 0.9798168540000916\n",
      "Epoch: 0/10 Step: 415/135000 Loss: 2.693739175796509\n",
      "Epoch: 0/10 Step: 416/135000 Loss: 0.8092392683029175\n",
      "Epoch: 0/10 Step: 417/135000 Loss: 1.8551405668258667\n",
      "Epoch: 0/10 Step: 418/135000 Loss: 1.6184552907943726\n",
      "Epoch: 0/10 Step: 419/135000 Loss: 1.5485069751739502\n",
      "Epoch: 0/10 Step: 420/135000 Loss: 1.179770588874817\n",
      "Epoch: 0/10 Step: 421/135000 Loss: 1.5974814891815186\n",
      "Epoch: 0/10 Step: 422/135000 Loss: 1.189705729484558\n",
      "Epoch: 0/10 Step: 423/135000 Loss: 1.6988568305969238\n",
      "Epoch: 0/10 Step: 424/135000 Loss: 0.7907782196998596\n",
      "Epoch: 0/10 Step: 425/135000 Loss: 1.4904752969741821\n",
      "Epoch: 0/10 Step: 426/135000 Loss: 1.6841400861740112\n",
      "Epoch: 0/10 Step: 427/135000 Loss: 1.5898871421813965\n",
      "Epoch: 0/10 Step: 428/135000 Loss: 3.0023062229156494\n",
      "Epoch: 0/10 Step: 429/135000 Loss: 1.7109169960021973\n",
      "Epoch: 0/10 Step: 430/135000 Loss: 0.6493660807609558\n",
      "Epoch: 0/10 Step: 431/135000 Loss: 6.172203063964844\n",
      "Epoch: 0/10 Step: 432/135000 Loss: 1.7115323543548584\n",
      "Epoch: 0/10 Step: 433/135000 Loss: 2.6599252223968506\n",
      "Epoch: 0/10 Step: 434/135000 Loss: 1.1736363172531128\n",
      "Epoch: 0/10 Step: 435/135000 Loss: 2.1466481685638428\n",
      "Epoch: 0/10 Step: 436/135000 Loss: 1.9242470264434814\n",
      "Epoch: 0/10 Step: 437/135000 Loss: 2.3128490447998047\n",
      "Epoch: 0/10 Step: 438/135000 Loss: 1.8424757719039917\n",
      "Epoch: 0/10 Step: 439/135000 Loss: 2.0646231174468994\n",
      "Epoch: 0/10 Step: 440/135000 Loss: 2.9287192821502686\n",
      "Epoch: 0/10 Step: 441/135000 Loss: 1.6930608749389648\n",
      "Epoch: 0/10 Step: 442/135000 Loss: 2.0785439014434814\n",
      "Epoch: 0/10 Step: 443/135000 Loss: 1.9317165613174438\n",
      "Epoch: 0/10 Step: 444/135000 Loss: 1.9654850959777832\n",
      "Epoch: 0/10 Step: 445/135000 Loss: 2.0835702419281006\n",
      "Epoch: 0/10 Step: 446/135000 Loss: 2.1583995819091797\n",
      "Epoch: 0/10 Step: 447/135000 Loss: 1.5982401371002197\n",
      "Epoch: 0/10 Step: 448/135000 Loss: 2.168646812438965\n",
      "Epoch: 0/10 Step: 449/135000 Loss: 2.3977835178375244\n",
      "Epoch: 0/10 Step: 450/135000 Loss: 1.596744179725647\n",
      "Epoch: 0/10 Step: 451/135000 Loss: 2.2831780910491943\n",
      "Epoch: 0/10 Step: 452/135000 Loss: 1.3739054203033447\n",
      "Epoch: 0/10 Step: 453/135000 Loss: 1.9749512672424316\n",
      "Epoch: 0/10 Step: 454/135000 Loss: 3.388807773590088\n",
      "Epoch: 0/10 Step: 455/135000 Loss: 2.5423834323883057\n",
      "Epoch: 0/10 Step: 456/135000 Loss: 2.566197395324707\n",
      "Epoch: 0/10 Step: 457/135000 Loss: 1.6647967100143433\n",
      "Epoch: 0/10 Step: 458/135000 Loss: 1.4826624393463135\n",
      "Epoch: 0/10 Step: 459/135000 Loss: 0.9165288209915161\n",
      "Epoch: 0/10 Step: 460/135000 Loss: 1.2979689836502075\n",
      "Epoch: 0/10 Step: 461/135000 Loss: 1.2523093223571777\n",
      "Epoch: 0/10 Step: 462/135000 Loss: 1.0376789569854736\n",
      "Epoch: 0/10 Step: 463/135000 Loss: 2.610696315765381\n",
      "Epoch: 0/10 Step: 464/135000 Loss: 1.5860416889190674\n",
      "Epoch: 0/10 Step: 465/135000 Loss: 1.6117101907730103\n",
      "Epoch: 0/10 Step: 466/135000 Loss: 1.8777539730072021\n",
      "Epoch: 0/10 Step: 467/135000 Loss: 2.087151527404785\n",
      "Epoch: 0/10 Step: 468/135000 Loss: 2.2280566692352295\n",
      "Epoch: 0/10 Step: 469/135000 Loss: 2.1539511680603027\n",
      "Epoch: 0/10 Step: 470/135000 Loss: 1.3154640197753906\n",
      "Epoch: 0/10 Step: 471/135000 Loss: 1.03144371509552\n",
      "Epoch: 0/10 Step: 472/135000 Loss: 1.462502360343933\n",
      "Epoch: 0/10 Step: 473/135000 Loss: 2.374230146408081\n",
      "Epoch: 0/10 Step: 474/135000 Loss: 1.6570324897766113\n",
      "Epoch: 0/10 Step: 475/135000 Loss: 2.4229161739349365\n",
      "Epoch: 0/10 Step: 476/135000 Loss: 1.3811094760894775\n",
      "Epoch: 0/10 Step: 477/135000 Loss: 1.6448769569396973\n",
      "Epoch: 0/10 Step: 478/135000 Loss: 1.212048888206482\n",
      "Epoch: 0/10 Step: 479/135000 Loss: 1.7729637622833252\n",
      "Epoch: 0/10 Step: 480/135000 Loss: 1.3047707080841064\n",
      "Epoch: 0/10 Step: 481/135000 Loss: 1.482243537902832\n",
      "Epoch: 0/10 Step: 482/135000 Loss: 1.8468739986419678\n",
      "Epoch: 0/10 Step: 483/135000 Loss: 2.6652920246124268\n",
      "Epoch: 0/10 Step: 484/135000 Loss: 1.6176884174346924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63108/2372252753.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/github.com/shiipou/coding-ai/.venv/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/github.com/shiipou/coding-ai/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(_epoch, 10):\n",
    "    for i, batch in enumerate(train_dataloader, start=index):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch[:, :1000]\n",
    "        output = model(batch, labels=batch)\n",
    "        loss = output[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save the model every 10 batche\n",
    "        if i % 10 == 0:\n",
    "            torch.save(model.state_dict(), './checkpoints/{}-{}.pt'.format(epoch, i))\n",
    "\n",
    "        print('Epoch: {}/{}'.format(epoch, 10),\n",
    "                'Step: {}/{}'.format(i, len(train_dataloader)),\n",
    "                'Loss: {}'.format(loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Kernel is dead",
     "output_type": "error",
     "traceback": [
      "Error: Kernel is dead",
      "at g._sendKernelShellControl (/home/shiishii/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1006305)",
      "at g.sendShellMessage (/home/shiishii/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1006074)",
      "at g.requestExecute (/home/shiishii/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1008616)",
      "at d.requestExecute (/home/shiishii/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:37:328037)",
      "at S.requestExecute (/home/shiishii/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:32:19306)",
      "at w.executeCodeCell (/home/shiishii/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300924)",
      "at w.execute (/home/shiishii/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300551)",
      "at w.start (/home/shiishii/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:296215)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at t.CellExecutionQueue.executeQueuedCells (/home/shiishii/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310950)",
      "at t.CellExecutionQueue.start (/home/shiishii/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    batch = batch[:, :1000]\n",
    "    output = model(batch, labels=batch)\n",
    "    loss = output[0]\n",
    "    print('Epoch: {}/{}, Test Loss: {}'.format(epoch, 10, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./out'):\n",
    "    os.mkdir('./out')\n",
    "torch.save(model.state_dict(), './out/gpt2_code_generator.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.generate_code(\"import torch\\n\", length=100, temperature=0.7, top_k=50)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25d5de7a11a9be3c45ed68958e595ac9c7ba08c6a06e3781997497fbdff2d650"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
